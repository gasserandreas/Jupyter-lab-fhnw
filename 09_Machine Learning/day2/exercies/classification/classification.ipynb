{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install ipywidgets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from ipywidgets import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Aufgabenblatt 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datensatz\n",
    "\n",
    "In diesem Aufgabenblatt verwenden wir den Iris-Datensatz.\n",
    "\n",
    "Der Iris-Datensatz wird in vielen Beispielen und Büchern über Classification in Machine Learning verwendet.\n",
    "Sie finden also online zusätzliche Informationen und Lösungs-Ansätze zu diesem Datensatz.\n",
    "Der Datensatz ist sogar in `sklearn` integriert, sprich wir können den Iris-Datensatz direkt über `sklearn.datasets`  laden.\n",
    "\n",
    "Der Iris-Datensatz ist hier beschrieben: https://en.wikipedia.org/wiki/Iris_flower_data_set\n",
    "\n",
    "| Feature            | Descriptiopn                                                                  |\n",
    "|--------------------|-------------------------------------------------------------------------------|\n",
    "| specie             | Spezie der Blume                                                              |\n",
    "| sepal length (cm)  | Länge des Kelchblattes der Blume (in Zentimeter).                             |\n",
    "| sepal width (cm)   | Breite des Kelchblattes der Blume (in Zentimeter).                            |\n",
    "| petal length (cm)  | Länge des Kronblattes der Blume (in Zentimeter).                              |\n",
    "| petal width (cm)   | Breite des Kelchblattes der Blume (in Zentimeter).                            |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ziel\n",
    "\n",
    "Das Ziel ist es anhand der Features vorhersagen zu können, um welche Blumenart es sich handelt.\n",
    "Wir sagen also basierend auf Inputs (`Features`) ein Output (`Label`, auch `Klasse`) voraus - wir machen also eine `Classification`.\n",
    "\n",
    "![Ziel des Aufgabenblattes](./img/goal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Im Setup sind bereits notwendige Schritte implementiert, die Sie im Aufgabenblatt 2 selbst implementierten.\n",
    "**Nach Aufgabenblatt 2 sollten die Schritte klar sein!**\n",
    "\n",
    "Wir laden hier die Daten über `sklearn.datasets`, und teilen anschliessend die Features in `X` und Zielvariable in `y` auf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# Laden des Iris-Datensatzes über sklearn.datasets\n",
    "iris = datasets.load_iris(as_frame=True)\n",
    "\n",
    "# Aufteilen der Daten in Features und Zielvariable\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# iris.target ist als Nummer abgespeichert (0, 1 oder 2), hier schlüsseln wir diese Codierung in die Spezien-Namen auf (target_names).\n",
    "y = y.apply(lambda key: iris.target_names[key]).rename('specie')\n",
    "\n",
    "display(HTML('X:'))\n",
    "display(X.head())\n",
    "display(HTML('y:'))\n",
    "display(y.to_frame().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analog zum Aufgabenblatt 2 machen wir hier ein `Test-Set`, `Validation-Set` und `Train-Set`, welche wir in den Aufgaben verwenden werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split in Test-Set and Data-Set\n",
    "X_data, X_test, y_data, y_test = train_test_split(X, y, random_state=2, stratify=y)\n",
    "# Split Data-Set in Train-Set and Validation-Set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_data, y_data, random_state=2, stratify=y_data)\n",
    "\n",
    "print(\"Test-Set\", X_test.shape)\n",
    "print(\"Data-Set\", X_data.shape)\n",
    "print(\"Train-Set\", X_train.shape)\n",
    "print(\"Validation-Set\", X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für `seaborn` in der Datenanalyse (Aufgabe 1) setzen wir `X_data` und `y_data` zu einem DataFrame `df_data` zusammen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_data = pd.concat([X_data, y_data.to_frame()], axis=1)\n",
    "\n",
    "display(df_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folgende Funktion müssen Sie in den Aufgaben verwenden. Die Umsetzung (der Code) der Funktion **muss nicht verstanden** werden. \n",
    "Die Funktion hilft die `Decision Region` eines Classifiers zu plotten. Wir verwenden die Funktion in späteren Aufgaben, um die Classifiers besser zu verstehen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model._base import LinearClassifierMixin\n",
    "import pandas as pd\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def plot_decision_regions(clf: LinearClassifierMixin, data: pd.DataFrame, x: str, y: str, colors = None, num_steps=1000):\n",
    "    \"\"\"\n",
    "    Funktion zum Plotten der Decision Region eines Classification Modells.\n",
    "    Es wird nicht erwartet, dass Sie die Umsetzung dieser Funktion verstehen.\n",
    "    :param clf: Classification Modell\n",
    "    :param data: Daten als DataFrame\n",
    "    :param x: Spaltenname der X-Achse\n",
    "    :param y: Spaltenname der Y-Achse\n",
    "    :param colors: Farben (optional)\n",
    "    :param num_steps: Auflösung des Decision Region Plots.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    xs = data[x]\n",
    "    ys = data[y]\n",
    "    x_min, x_max = xs.min() - 1, xs.max() + 1\n",
    "    y_min, y_max = ys.min() - 1, ys.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, abs(x_max - x_min) / num_steps), np.arange(y_min, y_max, abs(y_max - y_min) / num_steps))\n",
    "\n",
    "    zz = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    zz = zz.reshape(xx.shape)\n",
    "\n",
    "    if colors is not None:\n",
    "        # Map labels to color index for plt.contourf\n",
    "        zz = np.vectorize(lambda x: list(colors.keys()).index(x))(zz)\n",
    "        cmap = ListedColormap(list(colors.values()))\n",
    "        plt.contourf(xx, yy, zz, alpha=0.25, cmap=cmap, extend='both')\n",
    "    else:\n",
    "        zz = np.vectorize(lambda x: list(clf.classes_).index(x))(zz)\n",
    "        plt.contourf(xx, yy, zz, alpha=0.25, extend='both')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 1 - Daten Analyse\n",
    "\n",
    "Wir möchten Anhand unserer Features (wie `sepal length (cm)`) die Blumenspezie (`specie`) vorhersagen können.\n",
    "\n",
    "In Aufgabe 2 werden wir dafür ein Modell erstellen. In Aufgabe 1 werden wir erst einmal die Daten mit ein paar Plots genauer anschauen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aufgabe 1.1 - Histogram der Zielvariable\n",
    "\n",
    "Es macht immer Sinn, sich die **Verteilung der Zielvariable** anzuschauen.\n",
    "Bei einer `Classification` kann man einfach die Anzahl Datenpunkte pro `Klasse` ausgeben.\n",
    "\n",
    "1. Erstellen Sie ein Plot der Zielvariable `specie` mittels `sns.countplot` (oder `sns.histplot`)\n",
    "2. Interpretieren Sie den Plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 1.2 - `sns.stripplot`, `sns.violinplot`\n",
    "\n",
    "1. Erstellen Sie einen `scatter plot` vom `Data-Set` (`df_data`) für das Feature `sepal width (cm)` und der Klasse `specie` mittels `sns.stripplot`. Der Plot heisst `stripplot` weil es die Datenpunkte vom `scatter plot` in einem Streifen (engl. `stirp`) anordnet. Würden wir das nicht machen, könnten wir die Menge der Punkte schlechter beurteilen.\n",
    "2. Wiederholen Sie Schritt 1 für alle Features. Dies kann man mittels `for` Schleife (einfacher) oder mit einem `sns.PairGrid` (schwieriger) machen. Die Spalten des DataFrames kann man mit `df_data.columns` auslesen.\n",
    "3. Interpretieren Sie den erstellten Plot.\n",
    "4. Wiederholen Sie Schritt 2 mit dem `sns.violinplot`.\n",
    "5. (Extra) Es gibt noch weitere interessante Plots wie der `sns.swarmplot` oder den `sns.boxplot`.\n",
    "6. (Extra) Was ist der Unterschied von `sns.stripplot` und `sns.violinplot`?\n",
    "\n",
    "#### Hilfreiche Links\n",
    "\n",
    "* `sns.stripplot`: https://seaborn.pydata.org/generated/seaborn.stripplot.html\n",
    "* `sns.violinplot`: https://seaborn.pydata.org/generated/seaborn.violinplot.html\n",
    "* A complete guide to plotting categorical variables: https://towardsdatascience.com/a-complete-guide-to-plotting-categorical-variables-with-seaborn-bfe54db66bec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schlusswort Aufgabe 1\n",
    "\n",
    "Aufgabe 1 gibt Ihnen eine Idee der `Datenanalyse` für die `Classification`.\n",
    "Wir haben ein verschiedene hilfreiche Plots, wie den `countplot`, den `stripplot` und den `violinplot`  gesehen.\n",
    "Dies ist aber bei weitem nicht alles was man tun kann.\n",
    "Wir haben in Aufgabe 1.2 alle Features unabhängig von einander angeschaut, man könnte die Feature Interaktionen bereits in der Datenanalyse untersuchen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 2 - Logistic Regression\n",
    "\n",
    "In dieser Aufgabe erstellen wir unser erstes `Classification` Modell, ein `Logistic Regression` Modell.\n",
    "Die `LogisticRegression` ist ein `Classification`-Modell und **kein** `Regression`-Modell, trotz des irreführenden Namens!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 2.1 - Plot Features - `sns.scatterplot(..., hue='col')`\n",
    "\n",
    "Wieder aus **didaktischen Gründen** (analog zu Aufgabenblatt 2) verwenden wir hier **lediglich 2 Features**, so können wir unsere Datenpunkte und trainierten Modelle 2-dimensional visualisieren.\n",
    "Die X-Achse und Y-Achse sind jeweils die Features und die Farbe der Punkte (`hue`) ist das Label der Zielvariable.\n",
    "\n",
    "Wir verwenden hier die Features `petal length (cm)` und `petal width (cm)`.\n",
    "\n",
    "1. Visualisieren Sie die beiden Features `petal length (cm)` und `petal width (cm)` vom `Data-Set` (`df_data`) mittels `sns.scatterplot` und färben sie die Punkte mit dem `hue` Parameter nach der Zielvariable `specie` ein.\n",
    "2. In Aufgabe 2.2 trainieren wir eine `LogisticRegression`, was genau wird dieses Modell schlussendlich machen? Überlegen Sie sich es am Plot aus Schritt 1.\n",
    "3. (Extra) Was ist bei den Überlegungen aus Schritt 2 der Unterschied zur Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 2.2 - `sklearn.linear_model.LogisticRegression`\n",
    "\n",
    "Analog zur `LinearRegression` vom Aufgabenblatt 2, trainieren wir hier die `LogisticRegression`.\n",
    "\n",
    "1. Erstellen Sie eine `LogisticRegression` und trainieren Sie diese auf den Features `petal length (cm)` und `petal width (cm)` vom `Train-Set` (`X_train`, `y_train`).\n",
    "2. Predicten (`clf.predict`) Sie die Vorhersagen auf dem `Validation-Set` (`X_val`). Hier müssen Sie wieder die entsprechenden Features selektieren.\n",
    "3. Messen Sie die Genauigkeit (englisch Accuracy) mittels `sklearn.metrics.accuracy_score` von `y_val_hat` unseren Vorhersagen und `y_val` den tatsächlichen `Klassen`.\n",
    "4. Visualisieren Sie die `Decision Boundary` von unserem Modell mit der Hilfe von der oben definierten `plot_decision_regions` Funktion. Was zeigt dieser Plot?\n",
    "5. (Extra) In `sklearn.metrics` gibt es weitere Metriken, manche haben wir im Theorie Teil kennengelernt, wie den F1-Score (`f1_score`). Schauen Sie sich den `classification_report` an (Sammlung von solchen Metriken) und interpretieren Sie diese Metriken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 2.3 - Confusion Matrix\n",
    "\n",
    "In Aufgabe 2.2 haben wir ein Modell trainiert und die Genauigkeit gemessen.\n",
    "Oft ist es aber interessant, wo genau das Modell die Fehler macht.\n",
    "In der `Classification` bedeutet ein Fehler nämlich immer, dass wir einen Datenpunkt einer falschen `Klasse` zuordnen.\n",
    "Dafür haben wir im Theorie Teil die `Confusion Matrix` kennengelernt.\n",
    "\n",
    "Die Berechnung `Confusion Matrix` müssen wir nicht selbst programmieren, `sklearn` bietet bereits eine Implementierung dafür.\n",
    "\n",
    "1. Erstellen Sie mittels `sklearn.metrics.confusion_matrix` den Vorhersagen `y_val_hat` von Aufgabe 2.2 und den richtigen Labels `y_val` die `Confusion Matrix` auf dem `Validation-Set` und interpretieren Sie die `Confusion Matrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Extra) Aufgabe 2.4 - Was wurde gelernt?\n",
    "\n",
    "Die `Logistische Regression` hat folgende Form:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "    \\phi(x^{(i)}\\beta) = \\phi(\\beta_0 + x_1 \\beta_1 + \\cdots + x_p \\beta_p)\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "Dies ist einfach die `Lineare Regression` mit der Sigma-Funktion $\\phi$ als `Linker-Funktion`.\n",
    "\n",
    "Da wir nur zur Zeit nur zwei Features haben, vereinfacht sich das zu:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "    \\phi(x^{(i)}\\beta) = \\phi(\\beta_0 + x_1 \\beta_1 + x_2 \\beta_2)\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "Analog zum Aufgabenblatt 2 können wir auch nachschauen, welche $\\beta$s (Gewichte) gelernt wurden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"beta0\", clf.intercept_)\n",
    "print(\"beta1\", clf.coef_[:, 0])\n",
    "print(\"beta2\", clf.coef_[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es wurden drei verschiedene $\\beta_0$, $\\beta_1$ und $\\beta_2$ gelernt!\n",
    "\n",
    "1\\.\n",
    "Warum ist das? Was bedeutet das?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schlusswort Aufgabe 2\n",
    "\n",
    "In Aufgabe 2 haben wir unser erstes `Classification` Modell, eine `Logistische Regression`, angewandt.\n",
    "In den nächsten Aufgaben werden wir weitere `Classification` Modelle anwenden.\n",
    "\n",
    "Für die Auswertung unseres Modelles nutzten wir `Classification` Metriken, wie die `Genauigkeit` (Accuracy) \n",
    "und die `Confusion Matrix`.\n",
    "Diese Auswertungen können wir genau gleich wieder bei anderen Modellen anwenden.\n",
    "\n",
    "Der Iris-Datensatz ist bereits **linear gut separierbar**, mit nur 2 Featuren und ohne `Feature Engineering` und ohne nicht-lineare Modelle.\n",
    "Obwohl der Datensatz (zu) einfach ist bleiben wir auch für die weiteren Aufgaben auf dem Iris-Datensatz.\n",
    "Es geht in den weiteren Aufgaben mehr um die Visualisierungen der Modelle, sowie dem Verständnis wie sie sich unterscheiden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 3 - Support Vector Machine\n",
    "\n",
    "In Aufgabe 3 wenden wir `Support Vector Machinen` für die `Classification` an (`sklearn.svm.SVC`).\n",
    "`Support Vector Machinen` könnten auch für die Regression benutzt werden (`sklearn.svm.SVR`), daher der Name `SVC` im `sklearn`.\n",
    "\n",
    "### Aufgabe 3.1 - Linear Support Vector Machine\n",
    "\n",
    "Analog zur Aufgabe 2.2 trainieren wir unser Modell vorerst auf 2 Featuren: `petal length (cm)` und `petal width (cm)`.\n",
    "\n",
    "1. Erstellen Sie eine `sklearn.svm.SVC` mit dem linearen Kernel (`kernel='linear'`) und trainieren Sie diese auf den Features `petal length (cm)` und `petal width (cm)` vom `Train-Set` (`X_train`, `y_train`).\n",
    "2. Predicten (`clf.predict`) Sie die Vorhersagen auf dem `Validation-Set` (`X_val`). Hier müssen Sie wieder die entsprechenden Features selektieren.\n",
    "3. Messen Sie die Genauigkeit (englisch Accuracy) mittels `sklearn.metrics.accuracy_score` von unseren Vorhersagen `y_val_hat` und den tatsächlichen Labels `y_val`.\n",
    "4. Visualisieren Sie die decision boundary von unserem Modell mit der Hilfe von der oben definierten `plot_decision_regions` Funktion. Was zeigt dieser Plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 3.2 - Logistische Regression oder Support Vector Machine\n",
    "\n",
    "Im theoretischen Teil haben wir gesehen, wie die `Logistische Regression` und wie die `Support Vector Machine` hinter den Kullisen (innerhalb der `fit` Methode) trainiert werden.\n",
    "Nutzen Sie Ihr Wissen in folgender Aufgabe.\n",
    "\n",
    "Unten sehen Sie zwei Visualisierungen der `Decision Regions` von Aufgabe 2.2 und Aufgabe 3.1 \n",
    "\n",
    "Logistische Regression     |  Support Vector Machine\n",
    ":-------------------------:|:-------------------------:\n",
    "![Logistische Regression oder SVM?](./img/lr_or_svm_2.png) |  ![Logistische Regression oder SVM?](./img/lr_or_svm_1.png)  \n",
    "\n",
    "1. Woran man das das zugrundeliegende Modell anhand der `Decision Regions` erkennen?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 3.3 - Support Vector Machine - Kernel Trick - Nicht Lineares Modell\n",
    "\n",
    "Im theoretischen Teil haben wir den `Kernel Trick` erwähnt, den wir in der `Support Vector Machine` anwenden können, um ein nicht lineares Modell zu erhalten. Generell kann man diesen Trick in verschiedenen Modellen einsetzen (Lineare Regression, Logistische Regression, KNN, ...), er ist jedoch nur bei `Support Vector Machine` out-of-the-box in sklearn verfügbar.\n",
    "\n",
    "Welcher Kernel wir verwenden und welche Parameter wir für einen spezifischen Kernel verwenden sind `Hyper-Parameter` vom `Support Vector Machine` Modell.\n",
    "\n",
    "Wir können diese `Hyper-Parameter` manuell (mit Expertenwissen) setzen oder systematisch (mit `Hyper-Parameter Optimization` (Aufgabe 3.4)) finden.\n",
    "\n",
    "1. Erstellen Sie eine `sklearn.svm.SVC` mit dem `rbf` Kernel (`kernel='rbf'`) und trainieren Sie diese auf den Features `petal length (cm)` und `petal width (cm)` vom `Train-Set` (`X_train`, `y_train`).\n",
    "2. Predicten (`clf.predict`) Sie die Vorhersagen auf dem `Validation-Set` (`X_val`). Hier müssen Sie wieder die entsprechenden Features selektieren.\n",
    "3. Messen Sie die `Accuracy` mittels `sklearn.metrics.accuracy_score` von unseren Vorhersagen `y_val_hat` und den tatsächlichen Labels `y_val`.\n",
    "4. Visualisieren Sie die `Decision Boundary` von unserem Modell mit der Hilfe von der oben definierten `plot_decision_regions` Funktion. Was ist grundlegend anders als in der Visualisierung der `linearen Support Vector Machine` von Aufgabe 3.2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 3.4 - Support Vector Machine - Alle Features und Hyper-Parameter Optimization\n",
    "\n",
    "Nun möchten wir alle Features verwenden und den Hyper-Parameter `gamma` vom `rbf` Kernel mittels `Hyper-Parameter Optimierung` finden.\n",
    "\n",
    "Dazu können wir unterschiedliche Such-Strategien verwenden:\n",
    "* `Grid Search`: Systematisch ein Grid absuchen\n",
    "* `Randomized Search`: `n` Mal zufälle Parameter in einem fixen Bereich ausprobieren\n",
    "\n",
    "Wir verwenden hier `Randomized Search`, da es in der Praxis erstaunlich oft gut funktioniert.\n",
    "`sklearn` bietet bereits eine Implementierung mit `Cross Validation` für stabilere Aussagen für die einzelnen Parameter bereit namens `RandomizedSearchCV`.\n",
    "\n",
    "1. Erstellen Sie einen fixen Bereich für den `gamma` Parameter, analog zu diesem Beispiel.\n",
    "2. Erstellen Sie eine `RandomizedSearchCV` mit einer `SVC(kernel='rbf')`, dem in Schritt 1 erstellten fixen Bereich. Wählen Sie eine sinnvolle Anzahl an Iterationen (z.B. `n_iter=25`).\n",
    "3. Trainieren `fit` Sie die `RandomizedSearchCV` auf dem `Data-Set` (`X_data` `y_data`).\n",
    "4. Warum nehmen wir in Schritt 3 das `Data-Set` und nicht das `Train-Set`?\n",
    "5. Welches Hyper-Parameter wurden gefunden (hier `gamma`)? Verwenden Sie dazu `rscv.best_params_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Extra) Aufgabe 4 - Weitere Modelle\n",
    "\n",
    "Analog zur Aufgabe 2.2 (Logistische Regression) und Aufgabe 3.1 (Support Vector Machine), trainieren wir hier andere Classification-Modelle, um ein Gefühl für den Unterschied zu bekommen.\n",
    "\n",
    "### (Extra) Aufgabe 4.1 - RandomForestClassifier\n",
    "\n",
    "1. Erstellen Sie eine `sklearn.ensemble.RandomForestClassifier` und trainieren Sie diese auf den Features `petal length (cm)` und `petal width (cm)` vom `Train-Set` (`X_train`, `y_train`).\n",
    "2. Predicten (`clf.predict`) Sie die Vorhersagen auf dem `Validation-Set` (`X_val`). Hier müssen Sie wieder die entsprechenden Features selektieren.\n",
    "3. Messen Sie die Genauigkeit (englisch Accuracy) mittels `sklearn.metrics.accuracy_score` von unseren Vorhersagen `y_val_hat` und den tatsächlichen Labels `y_val`.\n",
    "4. Visualisieren Sie die decision boundary von unserem Modell mit der Hilfe von der oben definierten `plot_decision_regions` Funktion. Was zeigt dieser Plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Extra) Aufgabe 4.2 - KNN (K-Nearest-Neighbors)\n",
    "\n",
    "1. Erstellen Sie eine `sklearn.neighbors.KNeighborsClassifier` und trainieren Sie diese auf den Features `petal length (cm)` und `petal width (cm)` vom `Train-Set` (`X_train`, `y_train`).\n",
    "2. Predicten (`clf.predict`) Sie die Vorhersagen auf dem `Validation-Set` (`X_val`). Hier müssen Sie wieder die entsprechenden Features selektieren.\n",
    "3. Messen Sie die Genauigkeit (englisch Accuracy) mittels `sklearn.metrics.accuracy_score` von unseren Vorhersagen `y_val_hat` und den tatsächlichen Labels `y_val`.\n",
    "4. Visualisieren Sie die decision boundary von unserem Modell mit der Hilfe von der oben definierten `plot_decision_regions` Funktion. Was zeigt dieser Plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 5 - Test Set\n",
    "\n",
    "Im Setup haben wir das `Test-Set` (`X_test`, `y_test`) erstellt und beiseite gelegt. Nun wollen wir das gefundene Modell auf diesem `Test-Set` evaluieren.\n",
    "\n",
    "1. Nehmen Sie ein Modell aus einer vorherigen Aufgabe als finales Modell.\n",
    "2. Wenden Sie dieses Modell auf dem `Test-Set` (`X_test`, `y_test`) an. `X_test` muss allenfalls noch entsprechend verarbeitet werden.\n",
    "3. Bestimmen Sie die Genauigkeit auf den Vorhersagen von Schritt 2 und `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schlusswort Aufgabenblatt 3\n",
    "\n",
    "Im Aufgabenblatt 3 haben wir die `Classification` genauer angeschaut.\n",
    "Wir haben verschiedene Plots für die `Datenanalyse`, verschiedene `Metriken` und verschiedene Modelle angeschaut.\n",
    "Es ist vorallem wichtig, dass Sie ein erstes Gefühl für die `Classification` bekommen haben und die Unterschiede zur Regression (Aufgabenblatt 2) klar verstehen.\n",
    "\n",
    "Der verwendete Iris-Datensatz ist ein oft verwendeter erster Datensatz. In der Praxis hat man oft schwierigere Datensätze, dass man eine Genauigkeit von 100% erreicht ist eher unüblich und je nach Problem unmöglich."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
