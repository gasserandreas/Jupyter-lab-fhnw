{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with OneHotEncoding done manually (without a Pipeline)\n",
    "\n",
    "In this notebook we show how OneHotEncoding is done manually (without a Pipeline).\n",
    "\n",
    "We suggest to do OneHotEncoding in a Pipeline as done in `4_linear_regression.ipynb`, but this may show the mechanism behind OneHotEncoding better as well as being closer to the exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-09T07:42:00.030724Z",
     "iopub.status.busy": "2022-04-09T07:42:00.030427Z",
     "iopub.status.idle": "2022-04-09T07:42:00.667075Z",
     "shell.execute_reply": "2022-04-09T07:42:00.666252Z",
     "shell.execute_reply.started": "2022-04-09T07:42:00.030662Z"
    },
    "papermill": {
     "duration": 0.952664,
     "end_time": "2020-11-12T13:41:11.650914",
     "exception": false,
     "start_time": "2020-11-12T13:41:10.698250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.009145,
     "end_time": "2020-11-12T13:41:11.669935",
     "exception": false,
     "start_time": "2020-11-12T13:41:11.660790",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-09T07:42:00.668499Z",
     "iopub.status.busy": "2022-04-09T07:42:00.668053Z",
     "iopub.status.idle": "2022-04-09T07:42:00.726598Z",
     "shell.execute_reply": "2022-04-09T07:42:00.725769Z",
     "shell.execute_reply.started": "2022-04-09T07:42:00.668471Z"
    },
    "papermill": {
     "duration": 0.092779,
     "end_time": "2020-11-12T13:41:11.789446",
     "exception": false,
     "start_time": "2020-11-12T13:41:11.696667",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the train data\n",
    "train_data = pd.read_csv('../data/houses_train.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-09T07:42:00.729451Z",
     "iopub.status.busy": "2022-04-09T07:42:00.729202Z",
     "iopub.status.idle": "2022-04-09T07:42:00.734880Z",
     "shell.execute_reply": "2022-04-09T07:42:00.734225Z",
     "shell.execute_reply.started": "2022-04-09T07:42:00.729423Z"
    },
    "papermill": {
     "duration": 0.021139,
     "end_time": "2020-11-12T13:41:11.820019",
     "exception": false,
     "start_time": "2020-11-12T13:41:11.798880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split data into features and labels.\n",
    "X_data = train_data.drop(columns='price')\n",
    "y_data = train_data['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-09T07:42:00.736111Z",
     "iopub.status.busy": "2022-04-09T07:42:00.735824Z",
     "iopub.status.idle": "2022-04-09T07:42:00.759101Z",
     "shell.execute_reply": "2022-04-09T07:42:00.758443Z",
     "shell.execute_reply.started": "2022-04-09T07:42:00.736084Z"
    },
    "papermill": {
     "duration": 0.044926,
     "end_time": "2020-11-12T13:41:11.874527",
     "exception": false,
     "start_time": "2020-11-12T13:41:11.829601",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split features and labels into train (X_train, y_train) and validation set (X_val, y_val).\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_data, y_data, stratify=X_data['object_type_name'], test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.009172,
     "end_time": "2020-11-12T13:41:11.893381",
     "exception": false,
     "start_time": "2020-11-12T13:41:11.884209",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Define and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-09T07:42:00.760783Z",
     "iopub.status.busy": "2022-04-09T07:42:00.760536Z",
     "iopub.status.idle": "2022-04-09T07:42:00.797615Z",
     "shell.execute_reply": "2022-04-09T07:42:00.796898Z",
     "shell.execute_reply.started": "2022-04-09T07:42:00.760758Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we construct manually a DataFrame with the features `zipcode`, `object_type_name` and `municipality_name` one-hot-encoded.\n",
    "\n",
    "1. we define three `OneHotEncoders` on for `zipcode`, one for `object_type_name` and one for `municipality_name`.\n",
    "2. we \"train\" (`fit`) and apply (`transform`) them respectively on each feature. We can think of `fit` in the OneHotEncoder as fixating the mapping, e.g. which `zipcode` becomes the $i$th column in the output of the OneHotEncoder and `transform` as actually doing the OneHotEncoding.\n",
    "3. we combine the remaining numerical features with all features from the OneHotEncoders.\n",
    "\n",
    "We OneHotEncode the `zipcode` because even though it is a number it is a categorical feature. The `zipcode` `8000` is not bigger than (and not double of) the `zipcode` `4000`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-09T07:42:00.799091Z",
     "iopub.status.busy": "2022-04-09T07:42:00.798606Z",
     "iopub.status.idle": "2022-04-09T07:42:00.803103Z",
     "shell.execute_reply": "2022-04-09T07:42:00.802454Z",
     "shell.execute_reply.started": "2022-04-09T07:42:00.799064Z"
    },
    "papermill": {
     "duration": 0.009193,
     "end_time": "2020-11-12T13:41:11.931002",
     "exception": false,
     "start_time": "2020-11-12T13:41:11.921809",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "# 1. Define the OneHotEncoders\n",
    "ohe_zipcode = OneHotEncoder(handle_unknown='ignore')\n",
    "ohe_object_type_name = OneHotEncoder(handle_unknown='ignore')\n",
    "ohe_municipality_name = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Transform the zipcode to a string, otherwise sklearn warns us later.\n",
    "X_train['zipcode'] = X_train['zipcode'].astype(\"string\")\n",
    "\n",
    "# 2. Train and apply them on the individual feature.\n",
    "# zipcode\n",
    "X_train_ohe_zipcode = ohe_zipcode.fit_transform(X_train[['zipcode']])\n",
    "X_train_zipcode = pd.DataFrame(data=X_train_ohe_zipcode.toarray(), index=X_train.index, columns=ohe_zipcode.categories_[0])\n",
    "# object_type_name\n",
    "X_train_ohe_object_type_name = ohe_object_type_name.fit_transform(X_train[['object_type_name']])\n",
    "X_train_object_type_name = pd.DataFrame(data=X_train_ohe_object_type_name.toarray(), index=X_train.index, columns=ohe_object_type_name.categories_[0])\n",
    "# municipality_name\n",
    "X_train_ohe_municipality_name = ohe_municipality_name.fit_transform(X_train[['municipality_name']])\n",
    "X_train_municipality_name = pd.DataFrame(data=X_train_ohe_municipality_name.toarray(), index=X_train.index, columns=ohe_municipality_name.categories_[0])\n",
    "\n",
    "# 3. Combine the numerical features (e.g. `living_area`) together with the OneHotEncoder outputs\n",
    "X_train_ohe = pd.concat([\n",
    "    X_train.drop(columns=['zipcode', 'object_type_name', 'municipality_name']),  # numerical features\n",
    "    X_train_zipcode,  # zipcode OneHot features\n",
    "    X_train_object_type_name,  # object_type_name OneHot features\n",
    "    X_train_municipality_name  # municipality_name OneHot features\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "`X_train_ohe` has now all features with `zipcode`, `object_type_name` and `municipality_name` being one-hot-encoded."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18281, 4322)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_ohe.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can train our `LinearRegression` model on the `one-hot-encoded` data:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "_ = model.fit(X_train_ohe, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict and evaluate prices for the validation set\n",
    "\n",
    "The trained model will now be applied to the validation set. Note that we have to reuse the above `OneHotEncoders`, because we have to apply the same mapping \"learned\" in fit, so the columns are in the same order as during training.\n",
    "\n",
    "Therefore, we do the following steps:\n",
    "\n",
    "1. Prep validation data with OneHotEncoders from training.\n",
    "2. Predict prices for prepared validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-09T07:42:01.842420Z",
     "iopub.status.busy": "2022-04-09T07:42:01.842182Z",
     "iopub.status.idle": "2022-04-09T07:42:01.845870Z",
     "shell.execute_reply": "2022-04-09T07:42:01.845175Z",
     "shell.execute_reply.started": "2022-04-09T07:42:01.842391Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1686425.256605689\n"
     ]
    }
   ],
   "source": [
    "X_val['zipcode'] = X_val['zipcode'].astype(\"string\")\n",
    "\n",
    "# 1. Prep validation data\n",
    "X_val_ohe_zipcode = ohe_zipcode.transform(X_val[['zipcode']])\n",
    "X_val_zipcode = pd.DataFrame(data=X_val_ohe_zipcode.toarray(), index=X_val.index, columns=ohe_zipcode.categories_[0])\n",
    "X_val_ohe_object_type_name = ohe_object_type_name.transform(X_val[['object_type_name']])\n",
    "X_val_object_type_name = pd.DataFrame(data=X_val_ohe_object_type_name.toarray(), index=X_val.index, columns=ohe_object_type_name.categories_[0])\n",
    "X_val_ohe_municipality_name = ohe_municipality_name.transform(X_val[['municipality_name']])\n",
    "X_val_municipality_name = pd.DataFrame(data=X_val_ohe_municipality_name.toarray(), index=X_val.index, columns=ohe_municipality_name.categories_[0])\n",
    "\n",
    "X_val_ohe = X_val.drop(columns=['zipcode', 'object_type_name', 'municipality_name'])\n",
    "X_val_ohe = pd.concat([X_val_ohe, X_val_zipcode], axis=1)\n",
    "X_val_ohe = pd.concat([X_val_ohe, X_val_object_type_name], axis=1)\n",
    "X_val_ohe = pd.concat([X_val_ohe, X_val_municipality_name], axis=1)\n",
    "\n",
    "# 2. Predict for prepared validation data\n",
    "y_val_pred = model.predict(X_val_ohe)\n",
    "print(mean_absolute_percentage_error(y_val, y_val_pred))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Oh no what happend! Our model has a huge error! Our model is aweful!** All this work wasted...? Do we have a bug in the code?\n",
    "\n",
    "Looking at the performance on the `train set` shows that it is **not a bug**. We are good on the train data, but bad on new data (validation data), we **overfitted**! We overfitted extremly!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set: 26.039938676254344\n",
      "Val Set: 1686425.256605689\n",
      "------\n",
      "Number of features:  4322\n",
      "Number of samples:  18281\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = model.predict(X_train_ohe)\n",
    "print(\"Train Set:\", mean_absolute_percentage_error(y_train, y_train_pred))\n",
    "y_val_pred = model.predict(X_val_ohe)\n",
    "print(\"Val Set:\", mean_absolute_percentage_error(y_val, y_val_pred))\n",
    "\n",
    "print(\"------\")\n",
    "\n",
    "print(\"Number of features: \", len(X_train_ohe.columns))\n",
    "print(\"Number of samples: \", len(X_train_ohe))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We probably overfit due to having `4347 features` and only `18281 samples`. There is a rule of fist that you should have at least 10 times more samples than features.\n",
    "\n",
    "If we look at the learned $\\vec{\\beta}$, we can see that the learned values are huge positive and negative values, a typical property when overfitting."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.min(model.coef_)=-1072709201879814.9\n",
      "np.max(model.coef_)=21447039238626.008\n"
     ]
    }
   ],
   "source": [
    "print(f\"{np.min(model.coef_)=}\")\n",
    "print(f\"{np.max(model.coef_)=}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "What can we do?\n",
    "\n",
    "* Remove some Features\n",
    "* Regularization, so the model does not learn those huge $\\vec{\\beta}$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Remove some Features\n",
    "\n",
    "Here we only encode the first two numbers of the `zipcode` and `object_type_name` resulting in 104 new features rather than +4000. The `municipality_name` feature is just dropped (not used)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "# 1. Define the OneHotEncoders\n",
    "ohe_zipcode_2 = OneHotEncoder(handle_unknown='ignore')\n",
    "ohe_object_type_name = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Transform the zipcode to a string, otherwise sklearn warns us later.\n",
    "X_train['zipcode_2'] = (X_train['zipcode'].astype(\"int\") // 100).astype(\"string\")\n",
    "\n",
    "# 2. Train and apply them on the individual feature.\n",
    "# zipcode\n",
    "X_train_ohe_zipcode_2 = ohe_zipcode_2.fit_transform(X_train[['zipcode_2']])\n",
    "X_train_zipcode_2 = pd.DataFrame(data=X_train_ohe_zipcode_2.toarray(), index=X_train.index, columns=ohe_zipcode_2.categories_[0])\n",
    "# object_type_name\n",
    "X_train_ohe_object_type_name = ohe_object_type_name.fit_transform(X_train[['object_type_name']])\n",
    "X_train_object_type_name = pd.DataFrame(data=X_train_ohe_object_type_name.toarray(), index=X_train.index, columns=ohe_object_type_name.categories_[0])\n",
    "\n",
    "# 3. Combine the numerical features (e.g. `living_area`) together with the OneHotEncoder outputs\n",
    "X_train_ohe = pd.concat([\n",
    "    X_train.drop(columns=['zipcode', 'zipcode_2', 'object_type_name', 'municipality_name']),  # numerical features\n",
    "    X_train_zipcode_2,  # zipcode (first two digits only) OneHot features\n",
    "    X_train_object_type_name  # municipality_name OneHot features\n",
    "], axis=1)\n",
    "\n",
    "model = LinearRegression()\n",
    "_ = model.fit(X_train_ohe, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.13233061981303\n"
     ]
    }
   ],
   "source": [
    "X_val['zipcode_2'] = (X_val['zipcode'].astype(\"int\") // 100).astype(\"string\")\n",
    "\n",
    "# 1. Prep validation data\n",
    "X_val_ohe_zipcode_2 = ohe_zipcode_2.transform(X_val[['zipcode_2']])\n",
    "X_val_zipcode_2 = pd.DataFrame(data=X_val_ohe_zipcode_2.toarray(), index=X_val.index, columns=ohe_zipcode_2.categories_[0])\n",
    "X_val_ohe_object_type_name = ohe_object_type_name.transform(X_val[['object_type_name']])\n",
    "X_val_object_type_name = pd.DataFrame(data=X_val_ohe_object_type_name.toarray(), index=X_val.index, columns=ohe_object_type_name.categories_[0])\n",
    "\n",
    "X_val_ohe = pd.concat([\n",
    "    X_val.drop(columns=['zipcode', 'zipcode_2', 'object_type_name', 'municipality_name']),\n",
    "    X_val_zipcode_2,\n",
    "    X_val_object_type_name\n",
    "], axis=1)\n",
    "\n",
    "# 2. Predict for prepared validation data\n",
    "y_val_pred = model.predict(X_val_ohe)\n",
    "print(mean_absolute_percentage_error(y_val, y_val_pred))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now the value is reasonable. The model seems to overfit no more. Are at least not as obviously."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# Clean up, remove features added in above cell\n",
    "X_train = X_train.drop(columns=['zipcode_2'])\n",
    "X_val = X_val.drop(columns=['zipcode_2'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Regularization\n",
    "\n",
    "Another approach is to use regularization. We keep all features, but restrict the flexibility of the model. If a $\\beta$ becomes large, we punish the model in the cost function (see slides). This \"motivates\" the model to keep the $\\beta$ values small, which helps against the danger of overfitting."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.compose import make_column_transformer, TransformedTargetRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "numerical_features = list(X_train.drop(columns=['zipcode', 'object_type_name', 'municipality_name']).columns)\n",
    "\n",
    "model_regularized = Pipeline([\n",
    "    ('std', make_column_transformer([StandardScaler(), numerical_features], remainder='passthrough', sparse_threshold=0.0)),\n",
    "    ('reg', TransformedTargetRegressor(\n",
    "        regressor=Ridge(),  # regressor=LinearRegression(), # Try LinearRegression to see how big betas are even with StandardScaler on y\n",
    "        transformer=StandardScaler()\n",
    "    ))\n",
    "])\n",
    "_ = model_regularized.fit(X_train_ohe, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set: 32.80037545125168\n",
      "Val Set: 33.0673564906153\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = model_regularized.predict(X_train_ohe)\n",
    "print(\"Train Set:\", mean_absolute_percentage_error(y_train, y_train_pred))\n",
    "y_val_pred = model_regularized.predict(X_val_ohe)\n",
    "print(\"Val Set:\", mean_absolute_percentage_error(y_val, y_val_pred))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Regularzed: -0.9803635305174393 2.409262474770106\n"
     ]
    }
   ],
   "source": [
    "print(\"Model Regularzed:\", np.min(model_regularized['reg'].regressor_.coef_), np.max(model_regularized['reg'].regressor_.coef_))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyperparameter Selection for `alpha` with RandomizedSearchCV\n",
    "\n",
    "`Ridge` as the Hyperparameter `alpha` which represents the regularization strength (`alpha` is called `lambda` on slides).\n",
    "\n",
    "Rather than taking the default value of `alpha=1.0`, we can try out different values and pick the one that performs best.\n",
    "\n",
    "This can be done with `GridSearch` or `RandomizedSearch`.\n",
    "\n",
    "Here we use `GridSearchCV` which does a GridSearch and uses k-fold cross validation to measure how good a parameter is."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reg__regressor__alpha': 0.1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'reg__regressor__alpha': [0.01, 0.1, 1, 5, 10, 20, 50]\n",
    "}\n",
    "\n",
    "rs = GridSearchCV(model_regularized, param_grid, cv=3)\n",
    "_ = rs.fit(X_train_ohe, y_train)\n",
    "print(rs.best_params_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set: 32.87849947433423\n",
      "Val Set: 33.12139122338599\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = rs.predict(X_train_ohe)\n",
    "print(\"Train Set:\", mean_absolute_percentage_error(y_train, y_train_pred))\n",
    "y_val_pred = rs.predict(X_val_ohe)\n",
    "print(\"Val Set:\", mean_absolute_percentage_error(y_val, y_val_pred))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (Extra) Sparse vs. Dense\n",
    "\n",
    "An interesting special behavior of sklearn is that `LinearRegression` learns with a different optimization algorithm on sparse than on dense data.\n",
    "\n",
    "So if we transform `X_train_ohe` to be sparse, the model does not overfit (as hard)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set on model_sparse: 32.89321126280548\n",
      "Val Set on model_sparse: 33.13233061981303\n",
      "-------\n",
      "Train Set on model_sparse: 32.90761536725446\n",
      "Val Set on model_dense: 33.14018109827843\n"
     ]
    }
   ],
   "source": [
    "# Eval model trained on dense data\n",
    "model_dense = LinearRegression()\n",
    "_ = model_dense.fit(X_train_ohe, y_train)\n",
    "y_train_pred = model_dense.predict(X_train_ohe)\n",
    "print(\"Train Set on model_sparse:\", mean_absolute_percentage_error(y_train, y_train_pred))\n",
    "y_val_pred = model_dense.predict(X_val_ohe)\n",
    "print(\"Val Set on model_sparse:\", mean_absolute_percentage_error(y_val, y_val_pred))\n",
    "\n",
    "print(\"-------\")\n",
    "\n",
    "# Eval model trained on sparse data\n",
    "X_train_ohe_sparse = sparse.csr_matrix(X_train_ohe.to_numpy())  # Make data sparse\n",
    "model_sparse = LinearRegression()\n",
    "_ = model_sparse.fit(X_train_ohe_sparse, y_train)\n",
    "y_train_pred = model_sparse.predict(X_train_ohe.to_numpy())\n",
    "print(\"Train Set on model_sparse:\", mean_absolute_percentage_error(y_train, y_train_pred))\n",
    "y_val_pred = model_sparse.predict(X_val_ohe.to_numpy())\n",
    "print(\"Val Set on model_dense:\", mean_absolute_percentage_error(y_val, y_val_pred))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that the `LinearRegression` on dense data does clearly overfit the data.\n",
    "Note that the `LinearRegression` on sparse data does not clearly overfit the data.\n",
    "\n",
    "Even though `X_train_ohe_sparse` and `X_train_ohe` is **the same** data (with different representation) and the problem is convex (single best solution!) due to the different optimization algorithm different models are learned.\n",
    "\n",
    "This is very `sklearn` specific and there is no good theoretical reason why this is so. However it is interesting to point out."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "papermill": {
   "duration": 6.853286,
   "end_time": "2020-11-12T13:41:13.194189",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-11-12T13:41:06.340903",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
